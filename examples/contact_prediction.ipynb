{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "contact_prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kool7d/BioMakie.jl/blob/dev/examples/contact_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mEfDVMlOpkO"
      },
      "source": [
        "# Contact Prediction Examples\n",
        "\n",
        "This example shows contact prediction with the ESM-1b and MSA Transformer models. Contact prediction is based on a logistic regression over the model's attention maps, without any finetuning of the language model. This methodology is based on our ICLR 2021 paper, \"Transformer protein language models are unsupervised structure learners.\" (https://openreview.net/pdf?id=fylclEqgvgd). A pair of amino acids is defined to be in contact when Cb-Cb distance is within 8 angstroms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "sJp-HJMnOpkQ",
        "outputId": "4973fd15-44a6-4440-c1b4-f41294c8858d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import esm\n",
        "import torch\n",
        "import os\n",
        "from Bio import SeqIO\n",
        "import itertools\n",
        "from typing import List, Tuple\n",
        "import string"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9846aee57594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mBio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeqIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Bio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7GCR5toOpkR"
      },
      "source": [
        "torch.set_grad_enabled(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4L493S2OpkS"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "This sets up some sequence loading utilities for ESM-1b (`read_sequence`) and the MSA Transformer (`read_msa`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcaejNiDOpkS"
      },
      "source": [
        "# This is an efficient way to delete lowercase characters and insertion characters from a string\n",
        "deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
        "deletekeys[\".\"] = None\n",
        "deletekeys[\"*\"] = None\n",
        "translation = str.maketrans(deletekeys)\n",
        "\n",
        "def read_sequence(filename: str) -> Tuple[str, str]:\n",
        "    \"\"\" Reads the first (reference) sequences from a fasta or MSA file.\"\"\"\n",
        "    record = next(SeqIO.parse(filename, \"fasta\"))\n",
        "    return record.description, str(record.seq)\n",
        "\n",
        "def remove_insertions(sequence: str) -> str:\n",
        "    \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
        "    return sequence.translate(translation)\n",
        "\n",
        "def read_msa(filename: str, nseq: int) -> List[Tuple[str, str]]:\n",
        "    \"\"\" Reads the first nseq sequences from an MSA file, automatically removes insertions.\"\"\"\n",
        "    return [(record.description, remove_insertions(str(record.seq)))\n",
        "            for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjeI0VOFOpkS"
      },
      "source": [
        "## Run ESM-1b Contact Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQysRIUOOpkT"
      },
      "source": [
        "esm1b, esm1b_alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
        "esm1b = esm1b.eval().cuda()\n",
        "esm1b_batch_converter = esm1b_alphabet.get_batch_converter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuxAjwNcOpkT"
      },
      "source": [
        "esm1b_data = [\n",
        "    read_sequence(\"1a3a_1_A.a3m\"),\n",
        "    read_sequence(\"5ahw_1_A.a3m\"),\n",
        "    read_sequence(\"1xcr_1_A.a3m\"),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WOP8Oo8OpkU"
      },
      "source": [
        "esm1b_batch_labels, esm1b_batch_strs, esm1b_batch_tokens = esm1b_batch_converter(esm1b_data)\n",
        "esm1b_batch_tokens = esm1b_batch_tokens.cuda()\n",
        "print(esm1b_batch_tokens.size(), esm1b_batch_tokens.dtype)  # Should be a 2D tensor with dtype torch.int64."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKD27DgpOpkU"
      },
      "source": [
        "# %%time\n",
        "esm1b_contacts = esm1b.predict_contacts(esm1b_batch_tokens).cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI3FSyQZOpkV"
      },
      "source": [
        "fig, axes = plt.subplots(figsize=(18, 6), ncols=3)\n",
        "for ax, contact, sequence in zip(axes, esm1b_contacts, esm1b_batch_strs):\n",
        "    seqlen = len(sequence)\n",
        "    ax.imshow(contact[:seqlen, :seqlen], cmap=\"Blues\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd3jR0tKOpkV"
      },
      "source": [
        "## Run MSA Transformer Contact Prediction\n",
        "\n",
        "The MSAs used here are samples from the [`trRosetta` (v1) dataset](https://yanglab.nankai.edu.cn/trRosetta/benchmark/), also used in the MSA Transformer paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP3IJ4Y0OpkV"
      },
      "source": [
        "msa_transformer, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
        "msa_transformer = msa_transformer.eval().cuda()\n",
        "msa_batch_converter = msa_alphabet.get_batch_converter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOGbV2oXOpkW"
      },
      "source": [
        "msa_data = [\n",
        "    read_msa(\"1a3a_1_A.a3m\", 64),\n",
        "    read_msa(\"5ahw_1_A.a3m\", 64),\n",
        "    read_msa(\"1xcr_1_A.a3m\", 64),\n",
        "]\n",
        "msa_batch_labels, msa_batch_strs, msa_batch_tokens = msa_batch_converter(msa_data)\n",
        "msa_batch_tokens = msa_batch_tokens.cuda()\n",
        "print(msa_batch_tokens.size(), msa_batch_tokens.dtype)  # Should be a 3D tensor with dtype torch.int64."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRXN2QZcOpkW"
      },
      "source": [
        "# %%time\n",
        "msa_contacts = msa_transformer.predict_contacts(msa_batch_tokens).cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RemHoTMOpkW"
      },
      "source": [
        "fig, axes = plt.subplots(figsize=(18, 6), ncols=3)\n",
        "for ax, contact, msa in zip(axes, msa_contacts, msa_batch_strs):\n",
        "    seqlen = len(msa[0])\n",
        "    ax.imshow(contact[:seqlen, :seqlen], cmap=\"Blues\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}